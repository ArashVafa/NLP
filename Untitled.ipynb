{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.utils as ku \n",
    "import numpy as np \n",
    "\n",
    "reviewTexts_train = []\n",
    "sentiment_train = []\n",
    "\n",
    "reviewTexts_test = []\n",
    "sentiment_test = []\n",
    "\n",
    "with open('reviews_using_dataset.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',') \n",
    "    for i, row in enumerate(readCSV):\n",
    "        if i==0:\n",
    "            continue\n",
    "        if i < 100:\n",
    "            reviewTexts_train.append(row[0])\n",
    "            sentiment_train.append(int(row[1]))\n",
    "        else:\n",
    "            break\n",
    "            reviewTexts_test.append(row[0])\n",
    "            sentiment_test.append(int(row[1]))\n",
    "            \n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(reviewTexts_train)\n",
    "total_words = len(tokenizer.word_index)\n",
    "\n",
    "input_sequences = []\n",
    "for line in reviewTexts_train:\n",
    "    #generate token list for each line in the corpus\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    input_sequences.append(token_list)\n",
    "    \n",
    "#find the length of longest sentence. pad the rest of sequences to match max length \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "#add padding, by doing a pre-pad\n",
    "X_train = np.array(pad_sequences(input_sequences, maxlen=500, padding='pre'))\n",
    "\n",
    "\n",
    "\n",
    "input_sequences = []\n",
    "for line in reviewTexts_test:\n",
    "    #generate token list for each line in the corpus\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    input_sequences.append(token_list)\n",
    "    \n",
    "#add padding, by doing a pre-pad\n",
    "X_test = np.array(pad_sequences(input_sequences, maxlen=500, padding='pre'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#we will start with an embedding layer\n",
    "model.add(Embedding(total_words, 100, input_length=500))\n",
    "#next, let's add a bidirectional LSTM layer with 150 units. We use bidirectional because we want our LSTM to carry information both forwards and backwards\n",
    "\n",
    "################# TODO : fix the NN structure and run code ################################################# \n",
    "\n",
    "\n",
    "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
    "model.add(Dropout(0.2))\n",
    "#let's add another LSTM layer with 100 units\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "#the final layer should be a dense one, with total_words as the size. This is because we are interested in what the next word is, given the previous words, and we want the output to be a 1-hot encoding\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "#since this is a classification task, we will use categorical cross entropy as the loss function\n",
    "#adam optimizer works specially well for tasks of this type, so that is the optimizer that we use\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
